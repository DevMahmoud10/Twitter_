{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pprint\n",
    "import time\n",
    "from datetime import date\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareURL(hashtagName,tweetsList='top'):\n",
    "    url=\"https://twitter.com/hashtag/\"+hashtagName\n",
    "    if(tweetsList==\"latest\"):\n",
    "        url+=\"?f=tweets\"\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#t.tm_mday\n",
    "def extractLastTweetDate(soup):\n",
    "    s=soup.find_all('div', class_='content')\n",
    "    content=s[-1:]\n",
    "    tweetTime=content[0].find('a', class_=\"tweet-timestamp\")['title']\n",
    "    date=tweetTime.split(' ')\n",
    "    return str(date[3])+'-'+str(months.index(date[4])+1)+'-'+str(date[5]) #+1 for 0 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def requestPage(hashtagName,tweetsList):\n",
    "    url=prepareURL(hashtagName,tweetsList)\n",
    "    browser = webdriver.Chrome(\"/usr/lib/chromium-browser/chromedriver\")\n",
    "    browser.get(url)\n",
    "    while(True):\n",
    "        beforeScrollingPage=browser.page_source\n",
    "        if(tweetsList==\"latest\"):\n",
    "            sp=str(extractLastTweetDate(BeautifulSoup(beforeScrollingPage,\"html.parser\")))\n",
    "            sp=sp.split('-')\n",
    "            tweetDate=date(int(sp[2]),int(sp[1]), int(sp[0]))\n",
    "            todayDate=datetime.date.today()\n",
    "            delta=todayDate-tweetDate\n",
    "            if(delta.days>3):\n",
    "                break\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(10)\n",
    "        currentPage=browser.page_source\n",
    "        if(currentPage==beforeScrollingPage):\n",
    "            break\n",
    "    soup= BeautifulSoup(browser.page_source,\"html.parser\")\n",
    "    browser.quit()\n",
    "    open(\"page.html\",'w').write(str(soup))\n",
    "    return soup\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrapeHashtag(hashtagName,tweetsList):\n",
    "    #tweets list (top,latest)\n",
    "    soup=requestPage(hashtagName,tweetsList)\n",
    "    tweets=[]\n",
    "    for content in soup.find_all('div', class_='content'):\n",
    "        tweet={}\n",
    "        tweet[\"writer\"]=content.find('span', class_='username u-dir u-textTruncate').text.strip()\n",
    "        tweet[\"link\"]=content.find('a', class_=\"tweet-timestamp\")['href']\n",
    "        tweet[\"time\"]=content.find('a', class_=\"tweet-timestamp\")['title']\n",
    "        tweet[\"body\"]=content.find('p', class_=\"tweet-text\").text.replace('\\n','').strip()\n",
    "        #print(content.find('div', class_='ProfileTweet-action--reply').find)\n",
    "        tweetActions=content.find_all('span', class_='ProfileTweet-actionCount')\n",
    "        for action in tweetActions[:3]:\n",
    "            sp=action.text.strip().split(' ')\n",
    "            tweet[sp[1]]=sp[0]\n",
    "        #pprint.pprint(tweet)\n",
    "        #print('-'*50)\n",
    "        tweets.append(tweet)\n",
    "    df=pd.DataFrame(tweets).drop(['like','reply','retweet'],axis=1)\n",
    "    df.to_csv(hashtagName+\"_\"+tweetsList+\".csv\",index=False)\n",
    "    print(\"#\"+hashtagName+\" \"+tweetsList+\" contains \"+str(len(df))+\" tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scrapeHashtag(\"javascript\",\"top\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
